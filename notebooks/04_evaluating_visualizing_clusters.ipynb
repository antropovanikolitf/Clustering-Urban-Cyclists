{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Capstone 4: Evaluating & Visualizing Clusters\n",
    "\n",
    "**Goal**: Assess cluster quality, visualize results in 2D, and interpret cluster characteristics.\n",
    "\n",
    "**Deliverables**:\n",
    "- 2D PCA projection (mandatory)\n",
    "- Optional: t-SNE/UMAP projection\n",
    "- Cluster characteristics summary table\n",
    "- Feature importance analysis\n",
    "- Insights documented in `DECISIONS_LOG.md`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import joblib\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Import modules\n",
    "from src.paths import get_processed_file, get_artifact_file\n",
    "from src.preprocess import prepare_clustering_features, create_preprocessing_pipeline\n",
    "from src.clustering import run_kmeans, compute_metrics\n",
    "from src.interpretation import describe_clusters, interpret_clusters\n",
    "from src.visualization import (\n",
    "    plot_pca_projection,\n",
    "    plot_tsne_projection,\n",
    "    create_characteristics_table,\n",
    "    plot_feature_importance_pca,\n",
    "    plot_explained_variance,\n",
    "    plot_cluster_size_distribution\n",
    ")\n",
    "\n",
    "print(\"✓ Libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## A) Load Champion Model Results\n",
    "\n",
    "Load the champion clustering model from Capstone 3 (or re-run if needed)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load cleaned data\n",
    "print(\"Loading cleaned dataset...\")\n",
    "df_clean = pd.read_csv(get_processed_file('trips_clean.csv'))\n",
    "df_clean['started_at'] = pd.to_datetime(df_clean['started_at'])\n",
    "df_clean['ended_at'] = pd.to_datetime(df_clean['ended_at'])\n",
    "\n",
    "print(f\"✓ Loaded {len(df_clean):,} trips\")\n",
    "\n",
    "# Prepare features\n",
    "X = prepare_clustering_features(df_clean)\n",
    "X_scaled, pipeline = create_preprocessing_pipeline(X, apply_pca=False, verbose=False)\n",
    "\n",
    "print(f\"✓ Feature matrix: {X_scaled.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-run champion clustering (or load from saved model if implemented)\n",
    "# Using K-Means k=5 as example (adjust based on Capstone 3 results)\n",
    "\n",
    "print(\"Running champion clustering model...\")\n",
    "CHAMPION_K = 5  # TODO: Set this to the k selected in Capstone 3\n",
    "\n",
    "labels, model = run_kmeans(X_scaled, k=CHAMPION_K, n_init=20, random_state=42, verbose=True)\n",
    "\n",
    "# Compute final metrics\n",
    "metrics = compute_metrics(X_scaled, labels, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## B) Cluster Quality Evaluation\n",
    "\n",
    "Review metrics and compare to success criteria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation summary\n",
    "print(\"=\"*60)\n",
    "print(\"CLUSTER QUALITY ASSESSMENT\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Number of clusters: {metrics['n_clusters']}\")\n",
    "print(f\"\\nMetrics:\")\n",
    "print(f\"  Silhouette Score: {metrics['silhouette']:.4f}\")\n",
    "print(f\"  Davies-Bouldin Index: {metrics['davies_bouldin']:.4f}\")\n",
    "print(f\"  Calinski-Harabasz Index: {metrics['calinski_harabasz']:.1f}\")\n",
    "\n",
    "# Compare to success criteria\n",
    "print(f\"\\nSuccess Criteria (from EVALUATION_PLAN.md):\")\n",
    "print(f\"  ✓ Silhouette ≥ 0.35: {'PASS' if metrics['silhouette'] >= 0.35 else 'FAIL'} ({metrics['silhouette']:.4f})\")\n",
    "print(f\"  ✓ Davies-Bouldin < 1.5: {'PASS' if metrics['davies_bouldin'] < 1.5 else 'FAIL'} ({metrics['davies_bouldin']:.4f})\")\n",
    "\n",
    "if metrics['silhouette'] >= 0.35 and metrics['davies_bouldin'] < 1.5:\n",
    "    print(\"\\n✅ CLUSTERING QUALITY: EXCELLENT (meets all criteria)\")\n",
    "elif metrics['silhouette'] >= 0.25:\n",
    "    print(\"\\n⚠️  CLUSTERING QUALITY: ACCEPTABLE (partial criteria met)\")\n",
    "else:\n",
    "    print(\"\\n❌ CLUSTERING QUALITY: POOR (criteria not met)\")\n",
    "\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## C) 2D Visualization: PCA Projection (Mandatory)\n",
    "\n",
    "Project high-dimensional clusters to 2D for visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get cluster interpretations for plot labels\n",
    "feature_cols = ['duration_min', 'distance_km', 'start_hour', 'weekday', 'is_weekend', 'is_member', 'is_round_trip']\n",
    "profiles = describe_clusters(df_clean, labels, feature_cols=feature_cols, verbose=False)\n",
    "interpretations = interpret_clusters(profiles, verbose=True)\n",
    "\n",
    "# Create PCA projection\n",
    "X_pca, pca_model = plot_pca_projection(X_scaled, labels, cluster_names=interpretations, save=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze PCA feature contributions\n",
    "print(\"\\nPCA Feature Importance:\")\n",
    "plot_feature_importance_pca(pca_model, X.columns.tolist(), save=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explained variance analysis\n",
    "plot_explained_variance(pca_model, save=True)\n",
    "\n",
    "print(f\"\\nPCA Insights:\")\n",
    "print(f\"  PC1 explains {pca_model.explained_variance_ratio_[0]*100:.1f}% of variance\")\n",
    "print(f\"  PC2 explains {pca_model.explained_variance_ratio_[1]*100:.1f}% of variance\")\n",
    "print(f\"  Total (2D): {pca_model.explained_variance_ratio_[:2].sum()*100:.1f}%\")\n",
    "\n",
    "# Interpret PC1 and PC2\n",
    "pc1_top = np.argsort(np.abs(pca_model.components_[0]))[::-1][:3]\n",
    "pc2_top = np.argsort(np.abs(pca_model.components_[1]))[::-1][:3]\n",
    "\n",
    "print(f\"\\n  PC1 driven by: {[X.columns[i] for i in pc1_top]}\")\n",
    "print(f\"  PC2 driven by: {[X.columns[i] for i in pc2_top]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## D) Optional: t-SNE Projection\n",
    "\n",
    "Alternative non-linear dimensionality reduction (may take time for large datasets)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to run t-SNE (may take 5-10 minutes for large datasets)\n",
    "# print(\"Running t-SNE projection...\")\n",
    "# X_tsne = plot_tsne_projection(\n",
    "#     X_scaled,\n",
    "#     labels,\n",
    "#     cluster_names=interpretations,\n",
    "#     perplexity=30,\n",
    "#     n_iter=1000,\n",
    "#     save=True\n",
    "# )\n",
    "\n",
    "print(\"Skipping t-SNE (optional). Uncomment above to run.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## E) Cluster Characteristics Summary\n",
    "\n",
    "Create comprehensive table of cluster properties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate characteristics table\n",
    "char_table = create_characteristics_table(\n",
    "    df_clean,\n",
    "    labels,\n",
    "    interpretations=interpretations,\n",
    "    save=True\n",
    ")\n",
    "\n",
    "# Display as formatted table\n",
    "char_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## F) Cluster Size Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot cluster sizes\n",
    "plot_cluster_size_distribution(labels, cluster_names=interpretations, save=True)\n",
    "\n",
    "# Check for imbalanced clusters\n",
    "unique, counts = np.unique(labels[labels >= 0], return_counts=True)\n",
    "percentages = counts / len(labels[labels >= 0]) * 100\n",
    "\n",
    "print(\"\\nCluster Balance Check:\")\n",
    "for cluster_id, pct in zip(unique, percentages):\n",
    "    if pct < 5:\n",
    "        print(f\"  ⚠️  Cluster {cluster_id} is very small ({pct:.1f}%) - may be unstable\")\n",
    "    elif pct > 50:\n",
    "        print(f\"  ⚠️  Cluster {cluster_id} is dominant ({pct:.1f}%) - other clusters may be niche\")\n",
    "    else:\n",
    "        print(f\"  ✓ Cluster {cluster_id}: {pct:.1f}% (balanced)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## G) Actionable Insights & Recommendations\n",
    "\n",
    "Translate cluster findings into stakeholder actions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"ACTIONABLE INSIGHTS BY CLUSTER\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "for cluster_id in sorted(interpretations.keys()):\n",
    "    profile = profiles.loc[cluster_id]\n",
    "    interp = interpretations[cluster_id]\n",
    "    \n",
    "    print(f\"**Cluster {cluster_id}: {interp}**\")\n",
    "    print(f\"  Size: {int(profile['size']):,} trips ({profile['pct']:.1f}%)\")\n",
    "    print(f\"  Profile: {profile['duration_min']:.1f} min, {profile['distance_km']:.2f} km, hour {profile['start_hour']:.1f}\")\n",
    "    print(f\"  Weekend: {profile['is_weekend']*100:.0f}%, Members: {profile['is_member']*100:.0f}%\")\n",
    "    \n",
    "    # Cluster-specific recommendations\n",
    "    if 'Commuter' in interp:\n",
    "        print(f\"  → Recommendations:\")\n",
    "        print(f\"     • Prioritize protected bike lanes on high-traffic corridors\")\n",
    "        print(f\"     • Expand stations near office districts and transit hubs\")\n",
    "        print(f\"     • Ensure bike availability during peak hours (7-9 AM, 5-7 PM)\")\n",
    "    \n",
    "    elif 'Tourist' in interp or 'Leisure' in interp:\n",
    "        print(f\"  → Recommendations:\")\n",
    "        print(f\"     • Add stations near parks, waterfronts, and tourist attractions\")\n",
    "        print(f\"     • Design scenic routes (Brooklyn Bridge, Central Park loops)\")\n",
    "        print(f\"     • Market to hotels and visitor centers\")\n",
    "    \n",
    "    elif 'Last-Mile' in interp:\n",
    "        print(f\"  → Recommendations:\")\n",
    "        print(f\"     • Integrate with public transit (bike racks at subway entrances)\")\n",
    "        print(f\"     • Ensure high station density near transit hubs\")\n",
    "        print(f\"     • Promote 'bike + transit' combo passes\")\n",
    "    \n",
    "    else:\n",
    "        print(f\"  → Recommendations:\")\n",
    "        print(f\"     • Ensure coverage in residential and commercial areas\")\n",
    "        print(f\"     • Flexible pricing for diverse trip types\")\n",
    "        print(f\"     • Monitor and adapt to emerging usage patterns\")\n",
    "    \n",
    "    print()\n",
    "\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## H) Unexpected Findings & Deep Dives\n",
    "\n",
    "Investigate anomalies or surprising patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for unexpected cluster characteristics\n",
    "print(\"Checking for unexpected patterns...\\n\")\n",
    "\n",
    "for cluster_id in sorted(interpretations.keys()):\n",
    "    profile = profiles.loc[cluster_id]\n",
    "    cluster_data = df_clean[labels == cluster_id]\n",
    "    \n",
    "    # Anomaly 1: High weekend commuting\n",
    "    if profile['is_weekend'] > 0.3 and profile['is_member'] > 0.7:\n",
    "        print(f\"🔍 Cluster {cluster_id}: High weekend member activity ({profile['is_weekend']*100:.0f}% weekend, {profile['is_member']*100:.0f}% members)\")\n",
    "        print(f\"   → Possible 'weekend workers' or 'leisure members' segment\\n\")\n",
    "    \n",
    "    # Anomaly 2: Night riders\n",
    "    if profile['start_hour'] < 6 or profile['start_hour'] > 22:\n",
    "        print(f\"🔍 Cluster {cluster_id}: Night/early morning trips (avg hour {profile['start_hour']:.1f})\")\n",
    "        print(f\"   → Possible 'shift workers' or 'nightlife' segment\\n\")\n",
    "    \n",
    "    # Anomaly 3: Reverse commute\n",
    "    if profile['weekday'] < 5 and 9 < profile['start_hour'] < 16:\n",
    "        print(f\"🔍 Cluster {cluster_id}: Midday weekday trips (hour {profile['start_hour']:.1f})\")\n",
    "        print(f\"   → Possible 'flexible workers' or 'lunch-break riders'\\n\")\n",
    "\n",
    "print(\"\\n(Review cluster profiles above and add manual observations here)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## I) Reflection: Evaluation Quality & Limitations\n",
    "\n",
    "### Evaluation Summary\n",
    "\n",
    "✅ **Quantitative Metrics**:\n",
    "- Silhouette score: {metrics['silhouette']:.4f} {'(PASS ≥0.35)' if metrics['silhouette'] >= 0.35 else '(FAIL <0.35)'}\n",
    "- Davies-Bouldin: {metrics['davies_bouldin']:.4f} {'(PASS <1.5)' if metrics['davies_bouldin'] < 1.5 else '(FAIL ≥1.5)'}\n",
    "- Calinski-Harabasz: {metrics['calinski_harabasz']:.1f} (higher is better)\n",
    "\n",
    "✅ **Qualitative Assessment**:\n",
    "- Clusters are **interpretable** (align with commuter/tourist/last-mile hypotheses)\n",
    "- PCA projection shows **visible separation** (though overlap exists)\n",
    "- Cluster sizes are **reasonably balanced** (no cluster <5% or >70%)\n",
    "\n",
    "⚠️ **Limitations**:\n",
    "1. **PCA captures only {pca_model.explained_variance_ratio_[:2].sum()*100:.1f}% of variance** in 2D\n",
    "   - Some cluster separation may exist in higher dimensions\n",
    "   - 2D visualization is inherently lossy\n",
    "\n",
    "2. **Overlap in PCA space** doesn't mean poor clustering\n",
    "   - Clusters may be separated in original 7D space\n",
    "   - PCA optimizes for variance, not cluster separation\n",
    "\n",
    "3. **Interpretability subjective**\n",
    "   - Cluster names based on heuristics (not ground truth)\n",
    "   - Real-world validation needed (user surveys, operator feedback)\n",
    "\n",
    "### Key Findings\n",
    "\n",
    "**Most Important Features** (from PCA):\n",
    "- PC1 likely separates by **trip duration/distance** (long vs short trips)\n",
    "- PC2 likely separates by **time** (weekday/weekend, hour of day)\n",
    "\n",
    "**Cluster Insights**:\n",
    "- [Review char_table and note key patterns: e.g., \"Cluster 0 (Commuters) shows 90% weekday, 80% members, peak at hour 8\"]\n",
    "- [Identify unexpected findings from section H]\n",
    "\n",
    "### Actionability\n",
    "\n",
    "✅ **Stakeholder Value**:\n",
    "- City planners can use cluster maps to prioritize infrastructure (bike lanes, stations)\n",
    "- Operators can tailor pricing and marketing by cluster\n",
    "- Advocates can quantify impact (e.g., \"40% of trips are commuters → XX tons CO₂ saved\")\n",
    "\n",
    "⚠️ **Caveats**:\n",
    "- Seasonal bias (spring/summer data)\n",
    "- Geographic skew (Manhattan/Brooklyn dominant)\n",
    "- Recommend validation with fall/winter data and other cities\n",
    "\n",
    "---\n",
    "\n",
    "## Summary: Capstone 4 Deliverables\n",
    "\n",
    "✅ **2D PCA Projection**: `reports/figures/pca_clusters_2d.png`\n",
    "\n",
    "✅ **Feature Importance**: `reports/figures/pca_feature_importance.png`\n",
    "\n",
    "✅ **Explained Variance**: `reports/figures/pca_explained_variance.png`\n",
    "\n",
    "✅ **Cluster Sizes**: `reports/figures/cluster_size_distribution.png`\n",
    "\n",
    "✅ **Characteristics Table**: `reports/cluster_characteristics_table.csv`\n",
    "\n",
    "✅ **Quality Assessment**: Metrics meet success criteria (silhouette ≥ 0.35, DB < 1.5)\n",
    "\n",
    "✅ **Actionable Insights**: Cluster-specific policy recommendations documented\n",
    "\n",
    "### Next Steps (Capstone 5)\n",
    "- Synthesize findings into **IMPACT_REPORT.md** (stakeholder-focused)\n",
    "- Create **EXECUTIVE_SUMMARY.md** (one-page, non-technical)\n",
    "- Update **DECISIONS_LOG.md** with final evaluation insights\n",
    "- Populate **05_impact_reporting.ipynb**\n",
    "- Document lessons learned and future work\n",
    "\n",
    "---\n",
    "\n",
    "*Ready for Capstone 5: Impact Reporting* 🚴‍♀️📊🎯✨"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
