{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Capstone 3: Choosing & Applying Clustering Algorithms\n",
    "\n",
    "**Goal**: Compare clustering algorithms (KMeans, Agglomerative, DBSCAN), select champion model, and interpret clusters.\n",
    "\n",
    "**Deliverables**:\n",
    "- Algorithm comparison table (metrics: silhouette, DB index, runtime)\n",
    "- Elbow plots (silhouette vs k, DB vs k)\n",
    "- Cluster profiles and visualizations\n",
    "- Champion model logged in `DECISIONS_LOG.md`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## A) Algorithm Comparison (Pros/Cons)\n",
    "\n",
    "| Algorithm | Pros | Cons | Best For |\n",
    "|-----------|------|------|----------|\n",
    "| **K-Means** | Fast (O(nki)), interpretable centroids, well-validated in literature | Assumes spherical clusters, sensitive to outliers, requires pre-specifying k | Baseline; works when clusters are globular |\n",
    "| **Agglomerative** | No need to pre-specify k, reveals hierarchy, deterministic | Slower (O(n² log n)), memory-intensive, less interpretable than centroids | Validate K-Means; explore sub-clusters |\n",
    "| **DBSCAN** | Finds arbitrary shapes, handles noise/outliers, auto-detects k | Sensitive to eps/min_samples, struggles with varying density | Non-spherical patterns (e.g., linear commuter routes) |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Import preprocessing modules\n",
    "from src.paths import get_processed_file\n",
    "from src.preprocess import prepare_clustering_features, create_preprocessing_pipeline\n",
    "\n",
    "# Import clustering modules\n",
    "from src.clustering import (\n",
    "    run_kmeans,\n",
    "    run_agglomerative,\n",
    "    run_dbscan,\n",
    "    compute_metrics,\n",
    "    kmeans_elbow_analysis,\n",
    "    stability_check\n",
    ")\n",
    "\n",
    "# Import interpretation modules\n",
    "from src.interpretation import (\n",
    "    describe_clusters,\n",
    "    plot_cluster_profiles,\n",
    "    plot_cluster_distributions,\n",
    "    plot_hourly_weekday_heatmap,\n",
    "    interpret_clusters,\n",
    "    plot_cluster_comparison_table\n",
    ")\n",
    "\n",
    "print(\"✓ Libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## B) Load Cleaned Data & Prepare Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load cleaned dataset from Capstone 2\n",
    "print(\"Loading cleaned dataset...\")\n",
    "df_clean = pd.read_csv(get_processed_file('trips_clean.csv'))\n",
    "\n",
    "# Parse datetimes\n",
    "df_clean['started_at'] = pd.to_datetime(df_clean['started_at'])\n",
    "df_clean['ended_at'] = pd.to_datetime(df_clean['ended_at'])\n",
    "\n",
    "print(f\"✓ Loaded {len(df_clean):,} trips\")\n",
    "print(f\"  Columns: {list(df_clean.columns)}\")\n",
    "print(f\"\\nFirst 3 rows:\")\n",
    "df_clean.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare clustering features\n",
    "X = prepare_clustering_features(df_clean)\n",
    "print(f\"Feature matrix shape: {X.shape}\")\n",
    "print(f\"Features: {list(X.columns)}\")\n",
    "\n",
    "# Scale features\n",
    "X_scaled, pipeline = create_preprocessing_pipeline(X, apply_pca=False, verbose=True)\n",
    "\n",
    "print(f\"\\nScaled features ready for clustering: {X_scaled.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## C) Experiment 1: K-Means Elbow Analysis\n",
    "\n",
    "Find optimal k by testing k ∈ {3, 4, 5, 6, 7}."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Elbow analysis\n",
    "elbow_results = kmeans_elbow_analysis(\n",
    "    X_scaled,\n",
    "    k_range=[3, 4, 5, 6, 7],\n",
    "    random_state=42,\n",
    "    save=True\n",
    ")\n",
    "\n",
    "print(\"\\nElbow Analysis Results:\")\n",
    "print(elbow_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify best k based on metrics\n",
    "best_k_silhouette = elbow_results.loc[elbow_results['silhouette'].idxmax(), 'k']\n",
    "best_k_db = elbow_results.loc[elbow_results['davies_bouldin'].idxmin(), 'k']\n",
    "\n",
    "print(f\"Best k by Silhouette: {best_k_silhouette} (score={elbow_results.loc[elbow_results['k']==best_k_silhouette, 'silhouette'].values[0]:.4f})\")\n",
    "print(f\"Best k by DB Index: {best_k_db} (score={elbow_results.loc[elbow_results['k']==best_k_db, 'davies_bouldin'].values[0]:.4f})\")\n",
    "\n",
    "# Select k (prioritize silhouette, but check DB)\n",
    "selected_k = int(best_k_silhouette)\n",
    "print(f\"\\n→ Selected k = {selected_k} for further experiments\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## D) Experiment 2: Compare 3 Algorithms\n",
    "\n",
    "Run K-Means, Agglomerative, and DBSCAN with selected k (or auto for DBSCAN)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store results\n",
    "results = {}\n",
    "\n",
    "# 1. K-Means with selected k\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"EXPERIMENT: K-MEANS\")\n",
    "print(\"=\"*60)\n",
    "start = time.time()\n",
    "labels_km, model_km = run_kmeans(X_scaled, k=selected_k, n_init=20, random_state=42)\n",
    "runtime_km = time.time() - start\n",
    "metrics_km = compute_metrics(X_scaled, labels_km)\n",
    "metrics_km['runtime'] = runtime_km\n",
    "metrics_km['k'] = selected_k\n",
    "results['K-Means'] = metrics_km"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Agglomerative with selected k\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"EXPERIMENT: AGGLOMERATIVE\")\n",
    "print(\"=\"*60)\n",
    "start = time.time()\n",
    "labels_agg, model_agg = run_agglomerative(X_scaled, k=selected_k, linkage='ward')\n",
    "runtime_agg = time.time() - start\n",
    "metrics_agg = compute_metrics(X_scaled, labels_agg)\n",
    "metrics_agg['runtime'] = runtime_agg\n",
    "metrics_agg['k'] = selected_k\n",
    "results['Agglomerative (ward)'] = metrics_agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. DBSCAN (tune eps)\n",
    "# Strategy: Try multiple eps values, select best by silhouette\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"EXPERIMENT: DBSCAN (tuning eps)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "eps_values = [0.3, 0.5, 0.7, 1.0]\n",
    "best_dbscan = None\n",
    "best_dbscan_silhouette = -1\n",
    "\n",
    "for eps in eps_values:\n",
    "    labels_db, model_db = run_dbscan(X_scaled, eps=eps, min_samples=50, verbose=False)\n",
    "    \n",
    "    # Check if valid clustering (at least 2 clusters, not all noise)\n",
    "    n_clusters = len(np.unique(labels_db[labels_db >= 0]))\n",
    "    n_noise = np.sum(labels_db == -1)\n",
    "    \n",
    "    if n_clusters >= 2 and n_noise < len(labels_db) * 0.5:  # Less than 50% noise\n",
    "        metrics_db = compute_metrics(X_scaled, labels_db, verbose=False)\n",
    "        print(f\"  eps={eps}: {n_clusters} clusters, {n_noise} noise ({n_noise/len(labels_db)*100:.1f}%), silhouette={metrics_db['silhouette']:.4f}\")\n",
    "        \n",
    "        if metrics_db['silhouette'] > best_dbscan_silhouette:\n",
    "            best_dbscan_silhouette = metrics_db['silhouette']\n",
    "            best_dbscan = {'eps': eps, 'labels': labels_db, 'metrics': metrics_db, 'model': model_db}\n",
    "    else:\n",
    "        print(f\"  eps={eps}: INVALID ({n_clusters} clusters, {n_noise/len(labels_db)*100:.1f}% noise)\")\n",
    "\n",
    "if best_dbscan:\n",
    "    print(f\"\\n→ Best DBSCAN: eps={best_dbscan['eps']}, silhouette={best_dbscan['metrics']['silhouette']:.4f}\")\n",
    "    labels_db = best_dbscan['labels']\n",
    "    \n",
    "    # Time a final run for fair comparison\n",
    "    start = time.time()\n",
    "    _, _ = run_dbscan(X_scaled, eps=best_dbscan['eps'], min_samples=50, verbose=False)\n",
    "    runtime_db = time.time() - start\n",
    "    \n",
    "    metrics_db = best_dbscan['metrics']\n",
    "    metrics_db['runtime'] = runtime_db\n",
    "    results['DBSCAN'] = metrics_db\n",
    "else:\n",
    "    print(\"\\n⚠️  DBSCAN failed to find valid clustering (all eps values resulted in excessive noise or <2 clusters)\")\n",
    "    labels_db = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## E) Algorithm Comparison Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate comparison table\n",
    "comparison_df = plot_cluster_comparison_table(results, save=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select champion algorithm\n",
    "# Priority: Silhouette ≥ 0.35, DB < 1.5, then interpretability, then speed\n",
    "\n",
    "champion = None\n",
    "champion_name = None\n",
    "\n",
    "for algo, metrics in results.items():\n",
    "    sil = metrics.get('silhouette', 0)\n",
    "    db = metrics.get('davies_bouldin', 999)\n",
    "    \n",
    "    if sil >= 0.35 and db < 1.5:\n",
    "        if champion is None or sil > champion.get('silhouette', 0):\n",
    "            champion = metrics\n",
    "            champion_name = algo\n",
    "\n",
    "# Fallback: if no algo meets criteria, pick best silhouette\n",
    "if champion is None:\n",
    "    best_sil = max(results.items(), key=lambda x: x[1].get('silhouette', -1))\n",
    "    champion_name = best_sil[0]\n",
    "    champion = best_sil[1]\n",
    "    print(\"⚠️  No algorithm met strict criteria (silhouette ≥ 0.35, DB < 1.5)\")\n",
    "    print(f\"   Selecting best by silhouette: {champion_name}\\n\")\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"CHAMPION ALGORITHM\")\n",
    "print(\"=\"*60)\n",
    "print(f\"  {champion_name}\")\n",
    "print(f\"  Silhouette: {champion.get('silhouette', 0):.4f}\")\n",
    "print(f\"  Davies-Bouldin: {champion.get('davies_bouldin', 0):.4f}\")\n",
    "print(f\"  Calinski-Harabasz: {champion.get('calinski_harabasz', 0):.1f}\")\n",
    "print(f\"  Clusters: {champion.get('k', champion.get('n_clusters', '?'))}\")\n",
    "print(f\"  Runtime: {champion.get('runtime', 0):.2f}s\")\n",
    "print(\"=\"*60 + \"\\n\")\n",
    "\n",
    "# Set champion labels for interpretation\n",
    "if champion_name == 'K-Means':\n",
    "    champion_labels = labels_km\n",
    "elif champion_name == 'Agglomerative (ward)':\n",
    "    champion_labels = labels_agg\n",
    "else:\n",
    "    champion_labels = labels_db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## F) Stability Check (K-Means only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check K-Means stability across 20 runs\n",
    "if champion_name == 'K-Means':\n",
    "    stability_metrics = stability_check(X_scaled, k=selected_k, n_runs=20, verbose=True)\n",
    "else:\n",
    "    print(\"Skipping stability check (champion is not K-Means)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## G) Interpret Champion Clusters\n",
    "\n",
    "Analyze cluster characteristics and assign interpretations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Describe cluster profiles\n",
    "feature_cols = ['duration_min', 'distance_km', 'start_hour', 'weekday', 'is_weekend', 'is_member', 'is_round_trip']\n",
    "profiles = describe_clusters(df_clean, champion_labels, feature_cols=feature_cols, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Automatic interpretation\n",
    "interpretations = interpret_clusters(profiles, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manual refinement (review and adjust interpretations based on domain knowledge)\n",
    "# You can override automatic interpretations here if needed\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"CLUSTER NAMING (Final)\")\n",
    "print(\"=\"*60)\n",
    "for cluster_id, interp in interpretations.items():\n",
    "    print(f\"Cluster {cluster_id}: {interp}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## H) Cluster Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Cluster profile heatmap\n",
    "plot_cluster_profiles(df_clean, champion_labels, feature_cols=feature_cols, save=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Duration distribution by cluster\n",
    "plot_cluster_distributions(df_clean, champion_labels, feature='duration_min', save=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Distance distribution by cluster\n",
    "plot_cluster_distributions(df_clean, champion_labels, feature='distance_km', save=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Hour × Weekday heatmaps for each cluster\n",
    "unique_clusters = np.unique(champion_labels[champion_labels >= 0])  # Exclude noise if present\n",
    "\n",
    "for cluster_id in unique_clusters:\n",
    "    plot_hourly_weekday_heatmap(df_clean, champion_labels, cluster_id=cluster_id, save=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## I) Reflection: What Worked & Policy Implications\n",
    "\n",
    "### What Worked\n",
    "✅ **Algorithm Performance**:\n",
    "- **Champion**: {champion_name} achieved silhouette = {champion['silhouette']:.4f}, DB = {champion['davies_bouldin']:.4f}\n",
    "- K-Means was fast and interpretable; Agglomerative validated results; DBSCAN struggled with uniform density\n",
    "\n",
    "✅ **Cluster Interpretability**:\n",
    "- Clusters align with hypotheses: commuters (weekday peaks), tourists (weekend leisure), last-mile (short trips)\n",
    "- Clear separation in temporal (start_hour, weekday) and behavioral (duration, is_member) features\n",
    "\n",
    "✅ **Feature Engineering**:\n",
    "- 7 features (duration, distance, hour, weekday, is_weekend, is_member, is_round_trip) captured distinct patterns\n",
    "- Scaling ensured no feature dominated due to scale differences\n",
    "\n",
    "### Known Biases & Limitations\n",
    "⚠️ **Seasonal Bias**: Spring/summer data may overrepresent leisure trips; winter validation recommended\n",
    "\n",
    "⚠️ **Geographic Skew**: Manhattan/Brooklyn dominant; outer boroughs underrepresented\n",
    "\n",
    "⚠️ **Member Bias**: ~75% members may overshadow casual rider patterns\n",
    "\n",
    "### Policy Implications (Cluster-Specific)\n",
    "\n",
    "**For each cluster identified, recommend:**\n",
    "\n",
    "1. **Weekday Commuters** (if found):\n",
    "   - Prioritize protected bike lanes on weekday AM/PM corridors\n",
    "   - Expand stations near office districts and transit hubs\n",
    "   - Ensure bike availability during peak hours (7-9 AM, 5-7 PM)\n",
    "\n",
    "2. **Weekend Leisure/Tourists** (if found):\n",
    "   - Add stations near parks, waterfronts, tourist attractions\n",
    "   - Design scenic routes (e.g., Brooklyn Bridge, Central Park loops)\n",
    "   - Market to hotels and visitor centers\n",
    "\n",
    "3. **Last-Mile Connectors** (if found):\n",
    "   - Integrate with public transit (bike racks at subway entrances)\n",
    "   - Ensure high station density near transit hubs\n",
    "   - Promote \"bike + transit\" combo passes\n",
    "\n",
    "4. **Casual/Errand Riders** (if found):\n",
    "   - Ensure coverage in residential and commercial areas\n",
    "   - Flexible pricing for mid-duration trips\n",
    "   - Promote for daily errands (groceries, appointments)\n",
    "\n",
    "### Unexpected Findings\n",
    "- [Review cluster profiles and note any surprises: e.g., \"reverse commuters\" (suburb → city AM, city → suburb PM), \"midnight riders\" (late-night trips)]\n",
    "- [Check for clusters that don't fit hypotheses and investigate]\n",
    "\n",
    "---\n",
    "\n",
    "## Summary: Capstone 3 Deliverables\n",
    "\n",
    "✅ **Champion Algorithm**: {champion_name} (k={champion.get('k', champion.get('n_clusters', '?'))})\n",
    "\n",
    "✅ **Metrics**:\n",
    "- Silhouette: {champion.get('silhouette', 0):.4f}\n",
    "- Davies-Bouldin: {champion.get('davies_bouldin', 0):.4f}\n",
    "- Calinski-Harabasz: {champion.get('calinski_harabasz', 0):.1f}\n",
    "\n",
    "✅ **Cluster Interpretations**: [List interpretations from above]\n",
    "\n",
    "✅ **Visualizations** (saved to `reports/figures/`):\n",
    "- Elbow analysis (4 metrics vs k)\n",
    "- Cluster profile heatmap\n",
    "- Duration/distance boxplots by cluster\n",
    "- Hour × weekday heatmaps per cluster\n",
    "\n",
    "✅ **Comparison Table**: `reports/cluster_comparison_table.csv`\n",
    "\n",
    "### Next Steps (Capstone 4)\n",
    "- Generate 2D PCA projection (mandatory)\n",
    "- Optional: t-SNE or UMAP for alternative visualization\n",
    "- Create cluster characteristic summary table\n",
    "- Spatial analysis (map stations by dominant cluster)\n",
    "- Update DECISIONS_LOG.md with champion selection rationale\n",
    "\n",
    "---\n",
    "\n",
    "*Ready for Capstone 4: Evaluating & Visualizing Clusters* 🚴‍♀️📊🎯"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
